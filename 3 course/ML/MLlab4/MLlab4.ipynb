{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space size:  2\n",
      "State space size:  4\n",
      "\n",
      "States high values:\n",
      "4.8\n",
      "3.4028235e+38\n",
      "0.41887903\n",
      "3.4028235e+38\n",
      "\n",
      "States low values:\n",
      "-4.8\n",
      "-3.4028235e+38\n",
      "-0.41887903\n",
      "-3.4028235e+38\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.shape[0]\n",
    "print(\"Action space size: \", n_actions)\n",
    "print(\"State space size: \", n_states)\n",
    "\n",
    "print('\\nStates high values:')\n",
    "print(env.observation_space.high[0])\n",
    "print(env.observation_space.high[1])\n",
    "print(env.observation_space.high[2])\n",
    "print(env.observation_space.high[3])\n",
    "\n",
    "print('\\nStates low values:')\n",
    "print(env.observation_space.low[0])\n",
    "print(env.observation_space.low[1])\n",
    "print(env.observation_space.low[2])\n",
    "print(env.observation_space.low[3])\n",
    "\n",
    "\n",
    "buckets = (1, 1, 6, 12)\n",
    "\n",
    "upper_bounds = [\n",
    "        env.observation_space.high[0],\n",
    "        0.5,\n",
    "        env.observation_space.high[2],\n",
    "        math.radians(50)\n",
    "        ]\n",
    "lower_bounds = [\n",
    "        env.observation_space.low[0],\n",
    "        -0.5,\n",
    "        env.observation_space.low[2],\n",
    "        -math.radians(50)\n",
    "        ]\n",
    "\n",
    "n_episodes = 2000\n",
    "n_steps = 200\n",
    "min_alpha = 0.05\n",
    "min_epsilon = 0.05\n",
    "gamma = 0.99\n",
    "decay_rate = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 6, 12, 2)\n"
     ]
    }
   ],
   "source": [
    "Q = np.zeros(buckets + (n_actions,))\n",
    "print(np.shape(Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(obs):\n",
    "    ratios = [(obs[i] + abs(lower_bounds[i])) / (upper_bounds[i] - lower_bounds[i]) for i in range(len(obs))]\n",
    "    new_obs = [int(round((buckets[i] - 1) * ratios[i])) for i in range(len(obs))]\n",
    "    new_obs = [min(buckets[i] - 1, max(0, new_obs[i])) for i in range(len(obs))]\n",
    "    return tuple(new_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_strategy(state, eps):\n",
    "    random_exploration_exploitation = np.random.random()\n",
    "    if random_exploration_exploitation <= eps:\n",
    "        new_action = env.action_space.sample()\n",
    "    else:\n",
    "        new_action = np.argmax(Q[state])\n",
    "    return new_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_strategy(state):\n",
    "    return np.argmax(Q[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table(current_state, action, reward, new_state, alpha):\n",
    "    Q[current_state][action] += alpha * (reward + gamma * np.max(Q[new_state]) - Q[current_state][action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(n_ep, rewards):\n",
    "    x = range(n_ep)\n",
    "    plt.plot(x, rewards)\n",
    "    plt.xlabel('Episode number')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0/2000 failed with a total reward of: 16.0\n",
      "Episode:1/2000 failed with a total reward of: 11.0\n",
      "Episode:2/2000 failed with a total reward of: 15.0\n",
      "Episode:3/2000 failed with a total reward of: 11.0\n",
      "Episode:4/2000 failed with a total reward of: 16.0\n",
      "Episode:5/2000 failed with a total reward of: 26.0\n",
      "Episode:6/2000 failed with a total reward of: 11.0\n",
      "Episode:7/2000 failed with a total reward of: 20.0\n",
      "Episode:8/2000 failed with a total reward of: 14.0\n",
      "Episode:9/2000 failed with a total reward of: 16.0\n",
      "Episode:10/2000 failed with a total reward of: 14.0\n",
      "Episode:11/2000 failed with a total reward of: 14.0\n",
      "Episode:12/2000 failed with a total reward of: 19.0\n",
      "Episode:13/2000 failed with a total reward of: 51.0\n",
      "Episode:14/2000 failed with a total reward of: 34.0\n",
      "Episode:15/2000 failed with a total reward of: 23.0\n",
      "Episode:16/2000 failed with a total reward of: 27.0\n",
      "Episode:17/2000 failed with a total reward of: 14.0\n",
      "Episode:18/2000 failed with a total reward of: 21.0\n",
      "Episode:19/2000 failed with a total reward of: 20.0\n",
      "Episode:20/2000 failed with a total reward of: 18.0\n",
      "Episode:21/2000 failed with a total reward of: 17.0\n",
      "Episode:22/2000 failed with a total reward of: 35.0\n",
      "Episode:23/2000 failed with a total reward of: 19.0\n",
      "Episode:24/2000 failed with a total reward of: 15.0\n",
      "Episode:25/2000 failed with a total reward of: 16.0\n",
      "Episode:26/2000 failed with a total reward of: 19.0\n",
      "Episode:27/2000 failed with a total reward of: 28.0\n",
      "Episode:28/2000 failed with a total reward of: 39.0\n",
      "Episode:29/2000 failed with a total reward of: 18.0\n",
      "Episode:30/2000 failed with a total reward of: 15.0\n",
      "Episode:31/2000 failed with a total reward of: 26.0\n",
      "Episode:32/2000 failed with a total reward of: 22.0\n",
      "Episode:33/2000 failed with a total reward of: 20.0\n",
      "Episode:34/2000 failed with a total reward of: 15.0\n",
      "Episode:35/2000 failed with a total reward of: 39.0\n",
      "Episode:36/2000 failed with a total reward of: 26.0\n",
      "Episode:37/2000 failed with a total reward of: 28.0\n",
      "Episode:38/2000 failed with a total reward of: 12.0\n",
      "Episode:39/2000 failed with a total reward of: 18.0\n",
      "Episode:40/2000 failed with a total reward of: 32.0\n",
      "Episode:41/2000 failed with a total reward of: 29.0\n",
      "Episode:42/2000 failed with a total reward of: 26.0\n",
      "Episode:43/2000 failed with a total reward of: 53.0\n",
      "Episode:44/2000 failed with a total reward of: 14.0\n",
      "Episode:45/2000 failed with a total reward of: 76.0\n",
      "Episode:46/2000 failed with a total reward of: 24.0\n",
      "Episode:47/2000 failed with a total reward of: 30.0\n",
      "Episode:48/2000 failed with a total reward of: 36.0\n",
      "Episode:49/2000 failed with a total reward of: 35.0\n",
      "Episode:50/2000 failed with a total reward of: 21.0\n",
      "Episode:51/2000 failed with a total reward of: 27.0\n",
      "Episode:52/2000 failed with a total reward of: 17.0\n",
      "Episode:53/2000 failed with a total reward of: 21.0\n",
      "Episode:54/2000 failed with a total reward of: 46.0\n",
      "Episode:55/2000 failed with a total reward of: 75.0\n",
      "Episode:56/2000 failed with a total reward of: 13.0\n",
      "Episode:57/2000 failed with a total reward of: 39.0\n",
      "Episode:58/2000 failed with a total reward of: 39.0\n",
      "Episode:59/2000 failed with a total reward of: 23.0\n",
      "Episode:60/2000 failed with a total reward of: 34.0\n",
      "Episode:61/2000 failed with a total reward of: 12.0\n",
      "Episode:62/2000 failed with a total reward of: 62.0\n",
      "Episode:63/2000 failed with a total reward of: 19.0\n",
      "Episode:64/2000 failed with a total reward of: 38.0\n",
      "Episode:65/2000 failed with a total reward of: 83.0\n",
      "Episode:66/2000 failed with a total reward of: 35.0\n",
      "Episode:67/2000 failed with a total reward of: 67.0\n",
      "Episode:68/2000 failed with a total reward of: 13.0\n",
      "Episode:69/2000 failed with a total reward of: 47.0\n",
      "Episode:70/2000 failed with a total reward of: 82.0\n",
      "Episode:71/2000 failed with a total reward of: 70.0\n",
      "Episode:72/2000 failed with a total reward of: 30.0\n",
      "Episode:73/2000 failed with a total reward of: 28.0\n",
      "Episode:74/2000 failed with a total reward of: 103.0\n",
      "Episode:75/2000 failed with a total reward of: 32.0\n",
      "Episode:76/2000 failed with a total reward of: 12.0\n",
      "Episode:77/2000 failed with a total reward of: 14.0\n",
      "Episode:78/2000 failed with a total reward of: 90.0\n",
      "Episode:79/2000 failed with a total reward of: 26.0\n",
      "Episode:80/2000 failed with a total reward of: 26.0\n",
      "Episode:81/2000 failed with a total reward of: 27.0\n",
      "Episode:82/2000 failed with a total reward of: 40.0\n",
      "Episode:83/2000 failed with a total reward of: 113.0\n",
      "Episode:84/2000 failed with a total reward of: 45.0\n",
      "Episode:85/2000 failed with a total reward of: 27.0\n",
      "Episode:86/2000 failed with a total reward of: 14.0\n",
      "Episode:87/2000 failed with a total reward of: 20.0\n",
      "Episode:88/2000 failed with a total reward of: 17.0\n",
      "Episode:89/2000 failed with a total reward of: 13.0\n",
      "Episode:90/2000 failed with a total reward of: 62.0\n",
      "Episode:91/2000 failed with a total reward of: 18.0\n",
      "Episode:92/2000 failed with a total reward of: 19.0\n",
      "Episode:93/2000 failed with a total reward of: 33.0\n",
      "Episode:94/2000 failed with a total reward of: 131.0\n",
      "Episode:95/2000 failed with a total reward of: 95.0\n",
      "Episode:96/2000 failed with a total reward of: 39.0\n",
      "Episode:97/2000 failed with a total reward of: 69.0\n",
      "Episode:98/2000 failed with a total reward of: 90.0\n",
      "Episode:99/2000 failed with a total reward of: 56.0\n",
      "Episode:100/2000 failed with a total reward of: 37.0\n",
      "Episode:101/2000 failed with a total reward of: 66.0\n",
      "Episode:102/2000 failed with a total reward of: 35.0\n",
      "Episode:103/2000 failed with a total reward of: 66.0\n",
      "Episode:104/2000 failed with a total reward of: 98.0\n",
      "Episode:105/2000 failed with a total reward of: 108.0\n",
      "Episode:106/2000 failed with a total reward of: 65.0\n",
      "Episode:107/2000 failed with a total reward of: 70.0\n",
      "Episode:108/2000 failed with a total reward of: 30.0\n",
      "Episode:109/2000 failed with a total reward of: 35.0\n",
      "Episode:110/2000 failed with a total reward of: 186.0\n",
      "Episode:111/2000 failed with a total reward of: 22.0\n",
      "Episode:112/2000 failed with a total reward of: 157.0\n",
      "Episode:113/2000 failed with a total reward of: 40.0\n",
      "Episode:114/2000 failed with a total reward of: 24.0\n",
      "Episode:115/2000 failed with a total reward of: 117.0\n",
      "Episode:116/2000 failed with a total reward of: 48.0\n",
      "Episode:117/2000 failed with a total reward of: 61.0\n",
      "Episode:118/2000 failed with a total reward of: 87.0\n",
      "Episode:119/2000 failed with a total reward of: 137.0\n",
      "Episode:120/2000 failed with a total reward of: 132.0\n",
      "Episode:121/2000 failed with a total reward of: 55.0\n",
      "Episode:122/2000 failed with a total reward of: 45.0\n",
      "Episode:123/2000 failed with a total reward of: 23.0\n",
      "Episode:126/2000 failed with a total reward of: 124.0\n",
      "Episode:127/2000 failed with a total reward of: 61.0\n",
      "Episode:128/2000 failed with a total reward of: 72.0\n",
      "Episode:129/2000 failed with a total reward of: 45.0\n",
      "Episode:131/2000 failed with a total reward of: 87.0\n",
      "Episode:132/2000 failed with a total reward of: 169.0\n",
      "Episode:137/2000 failed with a total reward of: 69.0\n",
      "Episode:143/2000 failed with a total reward of: 161.0\n",
      "Episode:164/2000 failed with a total reward of: 139.0\n",
      "Episode:165/2000 failed with a total reward of: 60.0\n",
      "Episode:166/2000 failed with a total reward of: 24.0\n",
      "Episode:167/2000 failed with a total reward of: 38.0\n",
      "Episode:168/2000 failed with a total reward of: 24.0\n",
      "Episode:169/2000 failed with a total reward of: 159.0\n",
      "Episode:170/2000 failed with a total reward of: 107.0\n",
      "Episode:171/2000 failed with a total reward of: 61.0\n",
      "Episode:172/2000 failed with a total reward of: 180.0\n",
      "Episode:173/2000 failed with a total reward of: 47.0\n",
      "Episode:175/2000 failed with a total reward of: 51.0\n",
      "Episode:179/2000 failed with a total reward of: 153.0\n",
      "Episode:182/2000 failed with a total reward of: 128.0\n",
      "Episode:183/2000 failed with a total reward of: 49.0\n",
      "Episode:184/2000 failed with a total reward of: 55.0\n",
      "Episode:185/2000 failed with a total reward of: 123.0\n",
      "Episode:194/2000 failed with a total reward of: 52.0\n"
     ]
    }
   ],
   "source": [
    "rewards_for_episodes = []\n",
    "for episode in range(n_episodes):\n",
    "    current_state = env.reset()\n",
    "    current_state = discretize(current_state)\n",
    "    \n",
    "    #alpha = 0.2\n",
    "    #epsilon = 0.4\n",
    "    alpha = max(min_alpha, min(1.0, 1.0 - math.log10((episode + 1) / decay_rate)))\n",
    "    epsilon = max(min_epsilon, min(1.0, 1.0 - math.log10((episode + 1) / decay_rate)))\n",
    "\n",
    "    episode_reward_acc = 0\n",
    "\n",
    "    for t in range(n_steps):\n",
    "        #env.render()\n",
    "        action = epsilon_greedy_strategy(current_state, epsilon)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        new_state = discretize(new_state)\n",
    "        update_q_table(current_state, action, reward, new_state, alpha)\n",
    "        current_state = new_state\n",
    "\n",
    "        episode_reward_acc += reward\n",
    "\n",
    "        if done:\n",
    "            print('Episode:{}/{} failed with a total reward of: {}'.format(episode, n_episodes, episode_reward_acc))\n",
    "            break\n",
    "    #if episode_reward_acc == 200:\n",
    "        #alpha = 0.1\n",
    "        #epsilon = 0.1\n",
    "        #break\n",
    "    rewards_for_episodes.append(episode_reward_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Q_table.npy', Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_results(len(rewards_for_episodes), rewards_for_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test episode(with 200 steps) finished with a total reward of: 200.0\n"
     ]
    }
   ],
   "source": [
    "Q = np.load('Q_table.npy')\n",
    "\n",
    "current_state = env.reset()\n",
    "current_state = discretize(current_state)\n",
    "episode_reward = 0\n",
    "test_steps = 200\n",
    "\n",
    "for t in range(test_steps):\n",
    "    env.render()\n",
    "    action = greedy_strategy(current_state)\n",
    "    new_state, reward, done, _ = env.step(action)\n",
    "    new_state = discretize(new_state)\n",
    "    update_q_table(current_state, action, reward, new_state, min_alpha)\n",
    "    current_state = new_state\n",
    "    episode_reward += reward\n",
    "    time.sleep(0.02)\n",
    "\n",
    "    if done:\n",
    "        print('Test episode failed with a total reward of: {}'.format(episode_reward))\n",
    "        break\n",
    "\n",
    "print('Test episode(with {} steps) finished with a total reward of: {}'.format(test_steps, episode_reward))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
